{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name: Rohan vishwanath chatse \n",
    "Email: rohancrchatse@gmail.com \n",
    "Course: Full stack data science pro \n",
    "Git Link: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Explain the concept of forward propagation in a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Forward propagation in a neural network refers to the process of passing an input \n",
    "through the network to obtain an output or prediction. \n",
    "\n",
    "It involves calculating the activations of each layer starting from the input layer, \n",
    "passing through hidden layers, and finally producing an output at the output layer. \n",
    "\n",
    "The primary goal of forward propagation is to compute the network's output given a particular \n",
    "input, based on the current weights and biases.\n",
    "\n",
    "The process can be broken down into the following steps:\n",
    "\n",
    "1. Input Layer: The process starts with the input layer, where the network receives data \n",
    "(such as an image, text, or numerical values). \n",
    "Each input feature is passed to the corresponding neuron in the input layer.\n",
    "\n",
    "2. Weighted Sum: For each neuron in the hidden layer, a weighted sum of the inputs is computed. \n",
    "Each input feature is multiplied by a corresponding weight (which determines the importance of \n",
    "the feature), and then the bias term is added. \n",
    "\n",
    "Mathematically, this is expressed as:\n",
    "   \n",
    "   z = w1x1+ w2x2+ w3x3 +.....+ b\n",
    "   \n",
    "   where w1, w2, w3 are the weights, x1, x2, x3...xn  are the input features, and, b is the bias term.\n",
    "\n",
    "3. Activation Function: After computing the weighted sum ( z ), an activation function \n",
    "(such as ReLU, sigmoid, or tanh) is applied to determine the neuron's output. \n",
    "\n",
    "This function introduces non-linearity to the network, allowing it to model complex relationships \n",
    "between the input and output. \n",
    "The output of the activation function becomes the input to the next layer of neurons.\n",
    "\n",
    "4. Propagation Through Layers: This process is repeated across all layers of the network \n",
    "(hidden layers, if any), with the output of one layer serving as the input to the next. \n",
    "In each layer, the weighted sum is calculated and passed through the activation function.\n",
    "\n",
    "5. Output Layer: The final layer of the network is the output layer. \n",
    "It produces the network's prediction, which could be a class label (in classification tasks),\n",
    "a regression value, or other desired output. \n",
    "In a classification problem, for instance, the output might be a probability distribution \n",
    "across different classes, typically computed using a softmax function for multi-class classification.\n",
    "\n",
    "In summary, forward propagation is the process through which input data is passed forward through \n",
    "the network, layer by layer, until it reaches the output. \n",
    "During this process, each neuron computes a weighted sum of its inputs, applies an activation \n",
    "function, and passes the result to the next layer. \n",
    "Forward propagation allows the network to make predictions based on the current parameters (weights \n",
    "and biases).'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.What is the purpose of the activation function in forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The purpose of the activation function in forward propagation is to introduce non-linearity \n",
    "into the neural network, enabling it to model complex patterns and relationships within the data. \n",
    "\n",
    "Without activation functions, the network would essentially be a linear model, \n",
    "regardless of the number of layers, as the composition of linear functions is still linear. \n",
    "\n",
    "By applying an activation function after the weighted sum of inputs in each neuron, the network can \n",
    "learn and approximate non-linear functions, which is essential for solving more complex tasks \n",
    "like image recognition, natural language processing, and other real-world problems. \n",
    "\n",
    "Common activation functions, such as ReLU, sigmoid, and tanh, help transform the output of each \n",
    "neuron into a form that allows for better learning, feature extraction, and decision boundaries. \n",
    "\n",
    "This non-linearity also ensures that the network can learn hierarchical features and adapt to \n",
    "varying levels of abstraction across the layers, ultimately improving its ability to generalize and \n",
    "make accurate predictions.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Describe the steps involved in the backward propagation (backpropagation) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Backpropagation is the process used to train neural networks by adjusting the weights and biases \n",
    "to minimize the error in predictions. \n",
    "\n",
    "It begins with a forward pass, where input data is passed through the network to generate an output, \n",
    "and the error is calculated by comparing the predicted output to the actual target. \n",
    "\n",
    "In the backward pass, this error is propagated back through the network, layer by layer, to calculate \n",
    "how much each weight and bias contributed to the error. \n",
    "\n",
    "The gradients (or derivatives) of the loss with respect to the weights and biases are computed \n",
    "using the chain rule. T\n",
    "\n",
    "These gradients are then used to update the weights and biases, typically using an optimization \n",
    "algorithm like gradient descent, to reduce the error. \n",
    "\n",
    "This process is repeated for many iterations until the network's performance improves and the \n",
    "error decreases.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.What is the purpose of the chain rule in backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The purpose of the chain rule in backpropagation is to calculate the gradients of the loss \n",
    "function with respect to the weights and biases in a neural network. \n",
    "\n",
    "Backpropagation involves propagating the error backward through the network to update the parameters \n",
    "(weights and biases), but since each weight affects the final output through multiple layers of \n",
    "transformations, we need a way to compute how the loss changes with respect to each individual \n",
    "parameter in the network.\n",
    "\n",
    "The chain rule allows us to decompose the gradient calculation into smaller, manageable parts. \n",
    "Specifically, it helps us calculate the derivative of the loss with respect to the weights in each \n",
    "layer by \"chaining\" together the derivatives of the activations and the weights. \n",
    "\n",
    "For each layer, the chain rule ensures that we compute how much the error at the output depends \n",
    "on the error at the previous layer, which is critical for determining how each weight should be \n",
    "adjusted to minimize the overall loss. \n",
    "\n",
    "This makes it possible to efficiently compute gradients for deep networks with many layers, \n",
    "ensuring proper updates to each parameter during training.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Implement the forward propagation process for a simple neural network with one hidden layer using\n",
    "NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def forward_propagation(X, weights_input_hidden, weights_hidden_output, bias_hidden, bias_output):\n",
    "    \n",
    "    z_hidden = np.dot(X, weights_input_hidden) + bias_hidden  \n",
    "    a_hidden = relu(z_hidden)  \n",
    "    \n",
    "    z_output = np.dot(a_hidden, weights_hidden_output) + bias_output  \n",
    "    a_output = sigmoid(z_output)  \n",
    "    \n",
    "    return a_hidden, a_output  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
