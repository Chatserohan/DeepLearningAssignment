{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name: Rohan vishwanath chatse \n",
    "Email: rohancrchatse@gmail.com \n",
    "Course: Full stack data science pro \n",
    "Git link: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.What is the vanishing gradient problem in deep neural networks? How does it affect training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The vanishing gradient problem occurs when gradients become very small during backpropagation \n",
    "in deep neural networks, particularly with activation functions like sigmoid or tanh. \n",
    "As gradients are propagated backward through the layers, they shrink exponentially, making \n",
    "it difficult for the earlier layers to learn because their weights receive very small updates.\n",
    "\n",
    "This problem slows down training, prevents the network from learning effectively, and hinders \n",
    "convergence, especially in very deep networks.\n",
    "\n",
    "Solutions:\n",
    "- ReLU activation: Prevents gradients from vanishing because its gradient is constant (1) \n",
    "for positive inputs.\n",
    "- He initialization: Proper weight initialization helps maintain gradient flow.\n",
    "- Batch normalization: Stabilizes training and keeps gradients in a manageable range.\n",
    "- Residual networks (ResNets): Skip connections allow gradients to bypass certain layers.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Explain how Xavier initialization addresses the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Xavier initialization is a technique used to initialize the weights of neural networks \n",
    "in a way that helps mitigate the vanishing gradient problem. \n",
    "It is particularly useful when using activation functions like sigmoid or tanh, which are \n",
    "prone to saturating and causing gradients to vanish.\n",
    "\n",
    "How Xavier Initialization Works:\n",
    "\n",
    "1. Scaling the Weights: Xavier initialization sets the weights of the network to be drawn from \n",
    "a uniform or normal distribution with a mean of 0 and a variance that is \n",
    "inversely proportional to the number of inputs to the neuron \n",
    "\n",
    "2. Why This Helps:\n",
    "   - Preserves Variance Across Layers: Xavier initialization helps ensure that the variance of \n",
    "   the outputs of each layer stays roughly the same as the variance of its inputs. \n",
    "   This prevents the gradients from shrinking or exploding as they propagate through the network, \n",
    "   which helps address the vanishing gradient problem.\n",
    "   \n",
    "   - Stable Gradients: By balancing the scale of the weights, Xavier initialization helps avoid \n",
    "   extreme small or large gradients, ensuring more stable gradient flow during backpropagation, \n",
    "   especially when using activation functions like sigmoid or tanh that can saturate.\n",
    "\n",
    "\n",
    "Xavier initialization adjusts the weight scale based on the number of inputs and outputs to each \n",
    "layer, helping to maintain a stable variance for both activations and gradients. \n",
    "This reduces the likelihood of vanishing gradients, especially in deep networks, leading to more \n",
    "efficient training.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.What are some common activation functions that are prone to causing vanishing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Common activation functions that are prone to causing vanishing gradients include sigmoid \n",
    "and tanh. \n",
    "Both of these functions squash their input values to a small range, making their gradients \n",
    "very small when the inputs are large or small. \n",
    "\n",
    "For sigmoid, the output ranges from 0 to 1, and its gradient is very close to 0 when the output \n",
    "is near 0 or 1. \n",
    "\n",
    "Similarly, tanh squashes its outputs between -1 and 1, and its gradient becomes small when the \n",
    "output is close to -1 or 1. \n",
    "\n",
    "This causes the gradients to shrink significantly as they are propagated back through deep layers \n",
    "during training, which slows down learning, especially in very deep networks.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Define the exploding gradient problem in deep neural networks. How does it impact training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The exploding gradient problem occurs when the gradients during backpropagation become \n",
    "excessively large, especially in deep neural networks. \n",
    "This happens when the product of gradients across many layers grows exponentially, leading to \n",
    "very large updates to the weights. \n",
    "This can cause the model's parameters to change drastically, making the network unstable and \n",
    "difficult to train.\n",
    "\n",
    "Impact on Training:\n",
    "1. Instability: Large gradients can cause the model's weights to update too drastically, leading \n",
    "to a blow-up in the values of weights. \n",
    "This causes the network to diverge, making the training process fail to converge.\n",
    "   \n",
    "2. Numerical Issues: Extremely large gradients can result in numerical instabilities, such as \n",
    "overflow, where computations result in values too large to be represented by the computer, \n",
    "which can crash the training process or produce NaN (Not a Number) values.\n",
    "\n",
    "3. Slow Convergence: When gradients are too large, the model might overshoot the optimal \n",
    "solution during training, making the convergence slow or preventing it from reaching a good minimum.\n",
    "\n",
    "Common Causes:\n",
    "- Improper weight initialization: Large initial weights can cause large gradients.\n",
    "- Deep networks: With many layers, the gradients can grow exponentially if the network is not \n",
    "carefully designed.\n",
    "\n",
    "Solutions:\n",
    "- Gradient clipping: Limits the size of gradients to prevent them from exceeding a certain threshold.\n",
    "- Weight initialization: Proper initialization techniques like Xavier or He initialization help to \n",
    "prevent extreme gradient values.\n",
    "- Batch normalization: Helps stabilize the network by normalizing activations in each layer, \n",
    "reducing the likelihood of exploding gradients.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.What is the role of proper weight initialization in training deep neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Proper weight initialization is crucial in training deep neural networks because it helps \n",
    "maintain a stable and efficient learning process. \n",
    "When weights are initialized incorrectly, either too large or too small, it can cause issues \n",
    "like vanishing or exploding gradients, which hinder the networkâ€™s ability to learn. \n",
    "\n",
    "For instance, if weights are too small, the gradients may vanish as they propagate backward through \n",
    "the layers, making it hard for the network to update earlier layers. \n",
    "\n",
    "Conversely, if weights are too large, the gradients can explode, causing unstable updates and \n",
    "preventing convergence. \n",
    "\n",
    "Proper initialization, such as Xavier (Glorot) initialization for sigmoid or tanh activations, \n",
    "or He initialization for ReLU activations, helps to scale the weights so that the variance of \n",
    "the activations and gradients remains controlled throughout the network. \n",
    "\n",
    "This leads to faster convergence, better performance, and more stable training by ensuring that the \n",
    "gradients are neither too small nor too large.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Explain the concept of batch normalization and its impact on weight initialization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Batch normalization is a technique used to improve the stability and speed of training deep \n",
    "neural networks by normalizing the inputs to each layer. \n",
    "\n",
    "It works by adjusting the activations of each layer so that they have a mean of zero and a standard \n",
    "deviation of one, based on the statistics of each mini-batch. \n",
    "\n",
    "BN helps prevent issues like internal covariate shift, where the distribution of activations changes \n",
    "during training, leading to slow convergence. \n",
    "\n",
    "By normalizing the activations, BN keeps the gradients in a stable range, which reduces the risk of \n",
    "vanishing or exploding gradients. \n",
    "\n",
    "This makes the network less sensitive to the choice of weight initialization, as it ensures that \n",
    "activations remain controlled throughout the training process. \n",
    "As a result, BN allows for more flexible and less stringent weight initialization strategies, \n",
    "such as Xavier or He initialization, and helps the network converge faster, improving overall \n",
    "training efficiency and performance.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Implement He initialization in Python using TensorFlow or PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 19:38:06.855980: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-08 19:38:08.123949: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-08 19:38:10.561530: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10666 (41.66 KB)\n",
      "Trainable params: 10666 (41.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Dense(64, activation='relu', \n",
    "                          kernel_initializer=tf.keras.initializers.HeNormal(), \n",
    "                          input_shape=(128,)),  \n",
    "\n",
    "    \n",
    "    tf.keras.layers.Dense(32, activation='relu', \n",
    "                          kernel_initializer=tf.keras.initializers.HeNormal()),\n",
    "\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation='softmax') \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
